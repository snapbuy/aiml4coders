{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lYRdflY-Q14"
      },
      "source": [
        "# 19. 텐서플로 서빙으로 배포하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE2JANfj-Q16"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/rickiepark/aiml4coders/blob/main/ch19/19-serving_basic.ipynb\"><img src=\"https://jupyter.org/assets/share.png\" width=\"61\" />주피터 노트북 뷰어로 보기</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/aiml4coders/blob/main/ch19/19-serving_basic.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩(Colab)에서 실행하기</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Z-5Xp8hronXe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import json\n",
        "import tempfile\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bIqr4KZngTJ",
        "outputId": "b8fa3277-2924-4eec-bfd6-848cc53681a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2943  100  2943    0     0  94935      0 --:--:-- --:--:-- --:--:-- 94935\n",
            "OK\n",
            "Hit:1 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "67 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        }
      ],
      "source": [
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "!apt update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od7IZWGgn52G",
        "outputId": "18d63d82-53d9-4e4e-b74e-c7f7f472eb2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be upgraded:\n",
            "  tensorflow-model-server\n",
            "1 upgraded, 0 newly installed, 0 to remove and 66 not upgraded.\n",
            "Need to get 381 MB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 tensorflow-model-server all 2.9.0 [381 MB]\n",
            "Fetched 381 MB in 6s (60.7 MB/s)\n",
            "(Reading database ... 155640 files and directories currently installed.)\n",
            "Preparing to unpack .../tensorflow-model-server_2.9.0_all.deb ...\n",
            "Unpacking tensorflow-model-server (2.9.0) over (2.8.0) ...\n",
            "Setting up tensorflow-model-server (2.9.0) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install tensorflow-model-server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F73x9m5XE9l",
        "outputId": "ebbf2716-33fc-4e25-8289-524956f88eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-07-07 11:19:22--  http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-2.8.0/t/tensorflow-model-server/tensorflow-model-server_2.8.0_all.deb\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 74.125.197.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 340152790 (324M) [application/x-debian-package]\n",
            "Saving to: ‘tensorflow-model-server_2.8.0_all.deb.1’\n",
            "\n",
            "tensorflow-model-se 100%[===================>] 324.39M   182MB/s    in 1.8s    \n",
            "\n",
            "2022-07-07 11:19:24 (182 MB/s) - ‘tensorflow-model-server_2.8.0_all.deb.1’ saved [340152790/340152790]\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading tensorflow-model-server from 2.9.0 to 2.8.0\n",
            "(Reading database ... 155640 files and directories currently installed.)\n",
            "Preparing to unpack tensorflow-model-server_2.8.0_all.deb ...\n",
            "Unpacking tensorflow-model-server (2.8.0) over (2.9.0) ...\n",
            "Setting up tensorflow-model-server (2.8.0) ...\n"
          ]
        }
      ],
      "source": [
        "# 현재 코랩에서는 tensorflow-model-server 2.8.0 버전을 지원하므로 수동으로 패키지를 설치합니다.\n",
        "!wget 'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-2.8.0/t/tensorflow-model-server/tensorflow-model-server_2.8.0_all.deb'\n",
        "!dpkg -i tensorflow-model-server_2.8.0_all.deb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kkaYgOEn8KH",
        "outputId": "f160d61f-2c72-4e27-d719-a0b442e95741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished training the model\n",
            "[[18.982943]]\n"
          ]
        }
      ],
      "source": [
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(units=1, input_shape=[1])])\n",
        "\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='mean_squared_error')\n",
        "\n",
        "history = model.fit(xs, ys, epochs=500, verbose=0)\n",
        "\n",
        "print(\"Finished training the model\")\n",
        "\n",
        "print(model.predict([10.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxlUSdaFoAMP",
        "outputId": "03a959b5-4929-45de-8684-d131b9bef1a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/tmp/1\n"
          ]
        }
      ],
      "source": [
        "MODEL_DIR = tempfile.gettempdir()\n",
        "version = 1\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "print(export_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS2YypRFswST",
        "outputId": "30be804f-d126-461c-ff9d-472d3c0a4c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Already saved a model, cleaning up\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if os.path.isdir(export_path):\n",
        "    print('\\nAlready saved a model, cleaning up\\n')\n",
        "    !rm -r {export_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L8ZzWpptX7X",
        "outputId": "0a50e1b4-7d80-49ad-8571-9205c1adc505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/1/assets\n",
            "\n",
            "export_path = /tmp/1\n",
            "total 52\n",
            "drwxr-xr-x 2 root root  4096 Jul  7 11:19 assets\n",
            "-rw-r--r-- 1 root root  4092 Jul  7 11:19 keras_metadata.pb\n",
            "-rw-r--r-- 1 root root 37746 Jul  7 11:19 saved_model.pb\n",
            "drwxr-xr-x 2 root root  4096 Jul  7 11:19 variables\n"
          ]
        }
      ],
      "source": [
        "model.save(export_path, save_format=\"tf\")\n",
        "\n",
        "print('\\nexport_path = {}'.format(export_path))\n",
        "!ls -l {export_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E9OgCtxoK-t",
        "outputId": "4641fd40-faca-4b26-b390-73b25a3212cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: NoOp\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['dense_input'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 1)\n",
            "        name: serving_default_dense_input:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['dense'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 1)\n",
            "        name: StatefulPartitionedCall:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "\n",
            "Concrete Functions:\n",
            "  Function Name: '__call__'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          dense_input: TensorSpec(shape=(None, 1), dtype=tf.float32, name='dense_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          dense_input: TensorSpec(shape=(None, 1), dtype=tf.float32, name='dense_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "\n",
            "  Function Name: '_default_save_signature'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          dense_input: TensorSpec(shape=(None, 1), dtype=tf.float32, name='dense_input')\n",
            "\n",
            "  Function Name: 'call_and_return_all_conditional_losses'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          dense_input: TensorSpec(shape=(None, 1), dtype=tf.float32, name='dense_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          dense_input: TensorSpec(shape=(None, 1), dtype=tf.float32, name='dense_input')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          inputs: TensorSpec(shape=(None, 1), dtype=tf.float32, name='inputs')\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n"
          ]
        }
      ],
      "source": [
        "!saved_model_cli show --dir {export_path} --all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yj86AqUToOGv"
      },
      "outputs": [],
      "source": [
        "os.environ[\"MODEL_DIR\"] = MODEL_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM3L_b7xprB8",
        "outputId": "fd2ac1cb-a852-4788-dc12-48c46bb81f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting job # 0 in a separate thread.\n"
          ]
        }
      ],
      "source": [
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=helloworld \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1ebmCpj3X0ve"
      },
      "outputs": [],
      "source": [
        "# 서버가 시작되는 것을 기다리기 위해\n",
        "import time\n",
        "time.sleep(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCeFGrELoT6u",
        "outputId": "1b68b5f5-dff2-4240-ea27-82b1fd2d4f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[warn] getaddrinfo: address family for nodename not supported\n",
            "[evhttp_server.cc : 230] NET_LOG: Couldn't bind to port 8501\n",
            "[evhttp_server.cc : 63] NET_LOG: Server has not been terminated. Force termination now.\n",
            "[evhttp_server.cc : 265] NET_LOG: Server is not running ...\n",
            "2022-07-07 11:19:33.104917: E tensorflow_serving/model_servers/server.cc:440] Failed to start HTTP Server at localhost:8501\n"
          ]
        }
      ],
      "source": [
        "!tail server.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-USCEG6oWGn",
        "outputId": "0a528f25-3cbf-4da6-ee90-db1e7f0ef035"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"signature_name\": \"serving_default\", \"instances\": [[9.0], [10.0]]}\n"
          ]
        }
      ],
      "source": [
        "xs = np.array([[9.0], [10.0]])\n",
        "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": xs.tolist()})\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxs0ixtMoYpl",
        "outputId": "8d0cc35a-d0ec-436f-98f4-dca296d303e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"predictions\": [[16.9854145], [18.9829426]\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "headers = {\"content-type\": \"application/json\"}\n",
        "json_response = requests.post('http://localhost:8501/v1/models/helloworld:predict', data=data, headers=headers)\n",
        "\n",
        "print(json_response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbYRGO5qobNe",
        "outputId": "8fda1942-102b-41f1-ec59-ea135fc3c3d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[16.9854145], [18.9829426]]\n"
          ]
        }
      ],
      "source": [
        "predictions = json.loads(json_response.text)['predictions']\n",
        "print(predictions)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "serving_basic.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('tf29')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "972b08d88f25d6f55bd17f1fc0188a5326f0ce1bc5f1d2beca35e8c1590bd7b9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
